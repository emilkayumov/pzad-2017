{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAS competition (Home Credit). Прогнозирование невозврата кредита по кредитной истории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GroupKFold, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats = [np.max, np.min, np.mean, np.std, np.median, np.sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../input/'\n",
    "\n",
    "train = pd.read_csv(INPUT_PATH + 'train.csv')\n",
    "test = pd.read_csv(INPUT_PATH + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Внимание__. Датасет состоит из двух аналогичных частей, одна из которых рассчитывается по всем строкам выборки, а другая только по активным кредитным заявкам. Части далее конкатенируются. Для этого нужно запустить весь большой фрагмент ниже два раза (с новой загрузкой датасета) и закомментированием фильтрации по активным заявкам при втором проходе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с датами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.SK_DATE_DECISION = train.SK_DATE_DECISION.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%Y%m%d')\n",
    ")\n",
    "test.SK_DATE_DECISION = test.SK_DATE_DECISION.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%Y%m%d')\n",
    ")\n",
    "\n",
    "train.DTIME_CREDIT = train.DTIME_CREDIT.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%d.%m.%Y') if x is not None else None\n",
    ")\n",
    "test.DTIME_CREDIT = test.DTIME_CREDIT.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%d.%m.%Y') if x is not None else None\n",
    ")\n",
    "\n",
    "train.DTIME_CREDIT_ENDDATE = train.DTIME_CREDIT_ENDDATE.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%d.%m.%Y') if type(x) == str else None\n",
    ")\n",
    "test.DTIME_CREDIT_ENDDATE = test.DTIME_CREDIT_ENDDATE.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%d.%m.%Y') if type(x) == str else None\n",
    ")\n",
    "\n",
    "train.DTIME_CREDIT_ENDDATE_FACT = train.DTIME_CREDIT_ENDDATE_FACT.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%d.%m.%Y') if type(x) == str else None\n",
    ")\n",
    "test.DTIME_CREDIT_ENDDATE_FACT = test.DTIME_CREDIT_ENDDATE_FACT.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%d.%m.%Y') if type(x) == str else None\n",
    ")\n",
    "\n",
    "train.DTIME_CREDIT_UPDATE = train.DTIME_CREDIT_UPDATE.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%d.%m.%Y') if type(x) == str else None\n",
    ")\n",
    "test.DTIME_CREDIT_UPDATE = test.DTIME_CREDIT_UPDATE.apply(\n",
    "    lambda x: datetime.datetime.strptime(str(x), '%d.%m.%Y') if type(x) == str else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка простых весов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['min_credit_date'] = train.ID.map(train.groupby('ID')['DTIME_CREDIT'].min())\n",
    "test['min_credit_date'] = test.ID.map(test.groupby('ID')['DTIME_CREDIT'].min())\n",
    "\n",
    "train['delta_credit_date'] = train.DTIME_CREDIT - train.min_credit_date\n",
    "test['delta_credit_date'] = test.DTIME_CREDIT - test.min_credit_date\n",
    "\n",
    "train['delta_credit_date'] = train.delta_credit_date.apply(lambda x: x.days)\n",
    "test['delta_credit_date'] = test.delta_credit_date.apply(lambda x: x.days)\n",
    "\n",
    "train['weight'] = train.delta_credit_date.astype(float) / train.ID.map(train.groupby('ID')['delta_credit_date'].max())\n",
    "test['weight'] = test.delta_credit_date.astype(float) / test.ID.map(test.groupby('ID')['delta_credit_date'].max())\n",
    "\n",
    "train['sum_weight'] = train.ID.map(train.groupby('ID')['weight'].sum())\n",
    "test['sum_weight'] = test.ID.map(test.groupby('ID')['weight'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного кодирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train.CREDIT_CURRENCY = le.fit_transform(train.CREDIT_CURRENCY)\n",
    "test.CREDIT_CURRENCY = le.transform(test.CREDIT_CURRENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшая работа с выбросами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train.DTIME_CREDIT < datetime.datetime(1993, 9, 17), 'DTIME_CREDIT'] = datetime.datetime(1993, 9, 17)\n",
    "\n",
    "train.loc[626482, 'DTIME_CREDIT_UPDATE'] = train.loc[626481, 'DTIME_CREDIT_UPDATE']\n",
    "test.loc[1563857, 'DTIME_CREDIT_UPDATE'] = test.loc[1563856, 'DTIME_CREDIT_UPDATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['late_return'] = (train.DTIME_CREDIT_ENDDATE < train.DTIME_CREDIT_ENDDATE_FACT).astype(int)\n",
    "test['late_return'] = (test.DTIME_CREDIT_ENDDATE < test.DTIME_CREDIT_ENDDATE_FACT).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Внимание__. Следующую ячейку нужно закомментировать при повторном проходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only active\n",
    "train['is_current'] = (train.CREDIT_ACTIVE == 1).astype(int)\n",
    "test['is_current'] = (test.CREDIT_ACTIVE == 1).astype(int)\n",
    "\n",
    "train = train.loc[train.is_current == 1].reset_index(drop=True).copy()\n",
    "test = test.loc[test.is_current == 1].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(0, index=np.unique(train.ID), columns=[])\n",
    "df_test = pd.DataFrame(0, index=np.unique(test.ID), columns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ID'] = df_train.index\n",
    "df_test['ID'] = df_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для честных ctr по группам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=14)\n",
    "cv_ctr = GroupKFold(n_splits=5)\n",
    "\n",
    "groups = np.zeros(len(train))\n",
    "label = 0 \n",
    "for i_tr, i_ts in cv.split(df_train):\n",
    "    inds = df_train.iloc[i_ts]['ID']\n",
    "    groups[np.in1d(train.ID, inds)] = label\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['day_delta'] = (train.DTIME_CREDIT_ENDDATE - train.DTIME_CREDIT).dt.days\n",
    "train['day_delta_fact'] = (train.DTIME_CREDIT_ENDDATE - train.DTIME_CREDIT_ENDDATE_FACT).dt.days\n",
    "train['day_delta_update_start'] = (train.DTIME_CREDIT_UPDATE - train.DTIME_CREDIT).dt.days\n",
    "train['day_delta_update_end'] = (train.DTIME_CREDIT_UPDATE - train.DTIME_CREDIT_ENDDATE).dt.days\n",
    "\n",
    "test['day_delta'] = (test.DTIME_CREDIT_ENDDATE - test.DTIME_CREDIT).dt.days\n",
    "test['day_delta_fact'] = (test.DTIME_CREDIT_ENDDATE - test.DTIME_CREDIT_ENDDATE_FACT).dt.days\n",
    "test['day_delta_update_start'] = (test.DTIME_CREDIT_UPDATE - test.DTIME_CREDIT).dt.days\n",
    "test['day_delta_update_end'] = (test.DTIME_CREDIT_UPDATE - test.DTIME_CREDIT_ENDDATE).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ненужные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('CREDIT_COLLATERAL', axis=1)\n",
    "test = test.drop('CREDIT_COLLATERAL', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с текстовым признаком (количества символов), остальное позже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train['TEXT_PAYMENT_DISCIPLINE'].apply(lambda x: Counter(x) if type(x) == str else {})\n",
    "for char in ['C', 'X', '0', '1', '2', '3', '4', '5']:\n",
    "    train['num_{}'.format(char)] = tmp.apply(lambda x: x.get(char, 0))\n",
    "\n",
    "tmp = test['TEXT_PAYMENT_DISCIPLINE'].apply(lambda x: Counter(x) if type(x) == str else {})\n",
    "for char in ['C', 'X', '0', '1', '2', '3', '4', '5']:\n",
    "    test['num_{}'.format(char)] = tmp.apply(lambda x: x.get(char, 0))\n",
    "    \n",
    "train['num_other'] = train[['num_' + x for x in ['2', '3', '4', '5']]].sum(axis=1)\n",
    "test['num_other'] = test[['num_' + x for x in ['2', '3', '4', '5']]].sum(axis=1)\n",
    "\n",
    "for char in ['C', 'X', '0', '1', 'other']:\n",
    "    for func in fstats:\n",
    "        df_train['num_{}_{}'.format(char, func.__name__)] = df_train.ID.map(\n",
    "            train.groupby('ID')['num_{}'.format(char)].agg(func).fillna(0)\n",
    "        )\n",
    "        df_test['num_{}_{}'.format(char, func.__name__)] = df_test.ID.map(\n",
    "            test.groupby('ID')['num_{}'.format(char)].agg(func).fillna(0)\n",
    "        )\n",
    "        \n",
    "for char in ['C', 'X', '0', '1', 'other']:\n",
    "    train['tmp'] = train['num_{}'.format(char)] * train['weight']\n",
    "    test['tmp'] = test['num_{}'.format(char)] * test['weight']\n",
    "    df_train['num_{}_weighted'.format(char)] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test['num_{}_weighted'.format(char)] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Различные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['num_late_return'] = df_train.ID.map(train.groupby('ID')['late_return'].sum())\n",
    "df_test['num_late_return'] = df_test.ID.map(test.groupby('ID')['late_return'].sum())\n",
    "\n",
    "df_train['ratio_late_return'] = df_train.ID.map(train.groupby('ID')['late_return'].mean())\n",
    "df_test['ratio_late_return'] = df_test.ID.map(test.groupby('ID')['late_return'].mean())\n",
    "\n",
    "col = 'late_return'\n",
    "new_col = 'num_late_return_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['num_records'] = df_train.ID.map(train.groupby('ID').size())\n",
    "df_test['num_records'] = df_test.ID.map(test.groupby('ID').size())\n",
    "\n",
    "new_col = 'num_records_weighted'\n",
    "train['tmp'] = np.ones(len(train)) * train['weight']\n",
    "test['tmp'] = np.ones(len(test)) * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['DEF'] = df_train.ID.map(train.groupby('ID')['DEF'].first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = train.groupby('ID')['CREDIT_ACTIVE'].agg(lambda x: list(x))\n",
    "tmp = tmp.apply(lambda x: Counter(x))\n",
    "df_train['num_active'] = df_train.ID.map(tmp.apply(lambda x: x[0]))\n",
    "df_train['num_close'] = df_train.ID.map(tmp.apply(lambda x: x[1]))\n",
    "df_train['num_sell'] = df_train.ID.map(tmp.apply(lambda x: x[2]))\n",
    "df_train['num_bad'] = df_train.ID.map(tmp.apply(lambda x: x[3]))\n",
    "\n",
    "tmp = test.groupby('ID')['CREDIT_ACTIVE'].agg(lambda x: list(x))\n",
    "tmp = tmp.apply(lambda x: Counter(x))\n",
    "df_test['num_active'] = df_test.ID.map(tmp.apply(lambda x: x[0]))\n",
    "df_test['num_close'] = df_test.ID.map(tmp.apply(lambda x: x[1]))\n",
    "df_test['num_sell'] = df_test.ID.map(tmp.apply(lambda x: x[2]))\n",
    "df_test['num_bad'] = df_test.ID.map(tmp.apply(lambda x: x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ratio_active'] = df_train['num_active'] / df_train['num_records']\n",
    "df_train['ratio_close'] = df_train['num_close'] / df_train['num_records']\n",
    "\n",
    "df_test['ratio_active'] = df_test['num_active'] / df_test['num_records']\n",
    "df_test['ratio_close'] = df_test['num_close'] / df_test['num_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['day_delta', 'day_delta_fact', 'day_delta_update_start', 'day_delta_update_end']:\n",
    "    groups_train = train.groupby('ID')[col]\n",
    "    groups_test = test.groupby('ID')[col]\n",
    "    for func in fstats:\n",
    "        df_train['{}_{}'.format(col, func.__name__)] = df_train.ID.map(groups_train.agg(func))\n",
    "        df_test['{}_{}'.format(col, func.__name__)] = df_test.ID.map(groups_test.agg(func))\n",
    "        \n",
    "    new_col = col + '_weighted'\n",
    "    train['tmp'] = train[col] * train['weight']\n",
    "    test['tmp'] = test[col] * test['weight']\n",
    "    df_train[new_col] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test[new_col] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_train = train.groupby('ID')['CREDIT_DAY_OVERDUE']\n",
    "groups_test = test.groupby('ID')['CREDIT_DAY_OVERDUE']\n",
    "\n",
    "for func in fstats:\n",
    "    df_train['day_overdue_{}'.format(func.__name__)] = df_train.ID.map(groups_train.agg(func))\n",
    "    df_test['day_overdue_{}'.format(func.__name__)] = df_test.ID.map(groups_test.agg(func))\n",
    "\n",
    "    df_train['day_overdue_non_zero_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.loc[train['CREDIT_DAY_OVERDUE'] > 0].groupby('ID')['CREDIT_DAY_OVERDUE'].agg(func)\n",
    "    ).fillna(0)\n",
    "    df_test['day_overdue_non_zero_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.loc[test['CREDIT_DAY_OVERDUE'] > 0].groupby('ID')['CREDIT_DAY_OVERDUE'].agg(func)\n",
    "    ).fillna(0)\n",
    "\n",
    "\n",
    "col = 'CREDIT_DAY_OVERDUE'\n",
    "new_col = 'num_day_overdue_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.CREDIT_FACILITY.fillna(-1, inplace=True)\n",
    "test.CREDIT_FACILITY.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодирование категориальных признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_train.DEF.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.zeros(len(train))\n",
    "label = 0 \n",
    "for i_tr, i_ts in cv.split(df_train):\n",
    "    inds = df_train.iloc[i_ts]['ID']\n",
    "    groups[np.in1d(train.ID, inds)] = label\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = target.mean()\n",
    "alpha = 50.0\n",
    "cat_cols = ['CREDIT_TYPE', 'CREDIT_FACILITY', 'CREDIT_CURRENCY']\n",
    "\n",
    "for col in cat_cols:\n",
    "    test['ctr_'+col.lower()] = 0\n",
    "    for i_tr, i_ts in cv_ctr.split(train, groups=groups):\n",
    "        counts = train.iloc[i_tr].groupby(col).size()\n",
    "        means = train.iloc[i_tr].groupby(col)['DEF'].mean()\n",
    "        train.loc[i_ts, 'ctr_'+col.lower()] = train.iloc[i_ts][col].map(\n",
    "            (counts*means+alpha*global_mean)/(counts+alpha)\n",
    "        ).fillna(global_mean)\n",
    "        test['ctr_'+col.lower()] += test[col].map(\n",
    "            (counts*means+alpha*global_mean)/(counts+alpha)\n",
    "        ).fillna(global_mean) / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['ctr_credit_type', 'ctr_credit_facility', 'ctr_credit_currency']:\n",
    "    groups_train = train.groupby('ID')[col]\n",
    "    groups_test = test.groupby('ID')[col]\n",
    "    for func in fstats:\n",
    "        df_train['{}_{}'.format(col, func.__name__)] = df_train.ID.map(groups_train.agg(func))\n",
    "        df_test['{}_{}'.format(col, func.__name__)] = df_test.ID.map(groups_test.agg(func))\n",
    "        \n",
    "    new_col = col + '_weighted'\n",
    "    train['tmp'] = train[col] * train['weight']\n",
    "    test['tmp'] = test[col] * test['weight']\n",
    "    df_train[new_col] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test[new_col] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая часть работы с текстовым признаком (длины, доли, логрегрессия на tfidf и мешках слов, ...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_symbols(s, c):\n",
    "    inds = np.where(np.array(list(s))  == c)[0]\n",
    "    if len(inds) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sum(1.0/(inds+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train['TEXT_PAYMENT_DISCIPLINE'].copy()\n",
    "\n",
    "train['len_text_raw'] = train['TEXT_PAYMENT_DISCIPLINE'].fillna('').apply(len)\n",
    "test['len_text_raw'] = test['TEXT_PAYMENT_DISCIPLINE'].fillna('').apply(len)\n",
    "\n",
    "text.fillna('', inplace=True)\n",
    "text = text.apply(lambda x: \n",
    "                  filter(lambda y: y in ['0', '1', '2', '3', '4', '5', 'C', 'X'], x)\\\n",
    "                  .replace('3', '2').replace('4', '2').replace('5', '2'))\n",
    "cvect = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "train_features_tfidf = tfidf.fit_transform(text)\n",
    "train_features_cvect = cvect.fit_transform(text)\n",
    "train['len_text_clean'] = text.fillna('').apply(len)\n",
    "for c in ['0', '1', '2', 'C', 'X']:\n",
    "    train['golden_{}'.format(c)] = text.apply(lambda x: aggregate_symbols(x, c))\n",
    "    \n",
    "text = test['TEXT_PAYMENT_DISCIPLINE'].copy()\n",
    "text.fillna('', inplace=True)\n",
    "text = text.apply(lambda x: \n",
    "                  filter(lambda y: y in ['0', '1', '2', '3', '4', '5', 'C', 'X'], x)\\\n",
    "                  .replace('3', '2').replace('4', '2').replace('5', '2'))\n",
    "test_features_tfidf = tfidf.transform(text)\n",
    "test_features_cvect = cvect.transform(text)\n",
    "test['len_text_clean'] = text.fillna('').apply(len)\n",
    "for c in ['0', '1', '2', 'C', 'X']:\n",
    "    test['golden_{}'.format(c)] = text.apply(lambda x: aggregate_symbols(x, c))\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "train_features_cvect = scaler.fit_transform(train_features_cvect)\n",
    "test_features_cvect = scaler.transform(test_features_cvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cvect'] = 0\n",
    "train['tfidf'] = 0\n",
    "test['cvect'] = 0\n",
    "test['tfidf'] = 0\n",
    "\n",
    "for i_tr, i_ts in cv_ctr.split(train, groups=groups):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_features_cvect[i_tr], train.loc[i_tr, 'DEF'])\n",
    "    train.loc[i_ts, 'cvect'] = clf.predict_proba(train_features_cvect[i_ts])[:, 1]\n",
    "    test['cvect'] += clf.predict_proba(test_features_cvect)[:, 1] / 5.0\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_features_tfidf[i_tr], train.loc[i_tr, 'DEF'])\n",
    "    train.loc[i_ts, 'tfidf'] = clf.predict_proba(train_features_tfidf[i_ts])[:, 1]\n",
    "    test['tfidf'] += clf.predict_proba(test_features_tfidf)[:, 1] / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_features_cvect, test_features_cvect\n",
    "del train_features_tfidf, test_features_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in ['C', 'X', '0', '1', 'other']:\n",
    "    train['ratio_raw_{}'.format(char)] = train['num_{}'.format(char)].astype(float) / train['len_text_raw']\n",
    "    train['ratio_clean_{}'.format(char)] = train['num_{}'.format(char)].astype(float) / train['len_text_clean']\n",
    "    \n",
    "    test['ratio_raw_{}'.format(char)] = test['num_{}'.format(char)].astype(float) / test['len_text_raw']\n",
    "    test['ratio_clean_{}'.format(char)] = test['num_{}'.format(char)].astype(float) / test['len_text_clean']\n",
    "    \n",
    "    for func in fstats:\n",
    "        df_train['ratio_clean_{}_{}'.format(char, func.__name__)] = df_train.ID.map(\n",
    "            train.groupby('ID')['ratio_clean_{}'.format(char)].agg(func).fillna(0)\n",
    "        )\n",
    "        df_test['ratio_clean_{}_{}'.format(char, func.__name__)] = df_test.ID.map(\n",
    "            test.groupby('ID')['ratio_clean_{}'.format(char)].agg(func).fillna(0)\n",
    "        )\n",
    "        \n",
    "        df_train['ratio_raw_{}_{}'.format(char, func.__name__)] = df_train.ID.map(\n",
    "            train.groupby('ID')['ratio_raw_{}'.format(char)].agg(func).fillna(0)\n",
    "        )\n",
    "        df_test['ratio_raw_{}_{}'.format(char, func.__name__)] = df_test.ID.map(\n",
    "            test.groupby('ID')['ratio_raw_{}'.format(char)].agg(func).fillna(0)\n",
    "        )\n",
    "    \n",
    "    col = 'ratio_raw_{}'.format(char)\n",
    "    new_col = 'ratio_raw_{}'.format(char) + '_weighted'\n",
    "    train['tmp'] = train[col] * train['weight']\n",
    "    test['tmp'] = test[col] * test['weight']\n",
    "    df_train[new_col] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test[new_col] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    \n",
    "    col = 'ratio_clean_{}'.format(char)\n",
    "    new_col = 'ratio_clean_{}'.format(char) + '_weighted'\n",
    "    train['tmp'] = train[col] * train['weight']\n",
    "    test['tmp'] = test[col] * test['weight']\n",
    "    df_train[new_col] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test[new_col] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['tfidf', 'cvect', 'len_text_raw', 'len_text_clean']:\n",
    "    groups_train = train.groupby('ID')[col]\n",
    "    groups_test = test.groupby('ID')[col]\n",
    "    for func in fstats:\n",
    "        df_train['{}_{}'.format(col, func.__name__)] = df_train.ID.map(groups_train.agg(func))\n",
    "        df_test['{}_{}'.format(col, func.__name__)] = df_test.ID.map(groups_test.agg(func))\n",
    "        \n",
    "    new_col = col + '_weighted'\n",
    "    train['tmp'] = train[col] * train['weight']\n",
    "    test['tmp'] = test[col] * test['weight']\n",
    "    df_train[new_col] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test[new_col] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['0', '1', '2', 'C', 'X']:\n",
    "    groups_train = train.groupby('ID')['golden_{}'.format(c)]\n",
    "    groups_test = test.groupby('ID')['golden_{}'.format(c)]\n",
    "    for func in fstats:\n",
    "        df_train['golden_{}_{}'.format(c, func.__name__)] = df_train.ID.map(groups_train.agg(func))\n",
    "        df_test['golden_{}_{}'.format(c, func.__name__)] = df_test.ID.map(groups_test.agg(func))\n",
    "        \n",
    "    new_col = 'golden_{}'.format(c) + '_weighted'\n",
    "    train['tmp'] = train['golden_{}'.format(c)] * train['weight']\n",
    "    test['tmp'] = test['golden_{}'.format(c)] * test['weight']\n",
    "    df_train[new_col] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test[new_col] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё различные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['has_micro_credit'] = (train.CREDIT_TYPE == 19).astype(int)\n",
    "test['has_micro_credit'] = (test.CREDIT_TYPE == 19).astype(int)\n",
    "train['has_ipoteka'] = (train.CREDIT_TYPE == 3).astype(int)\n",
    "test['has_ipoteka'] = (test.CREDIT_TYPE == 3).astype(int)\n",
    "train['has_potreb'] = (train.CREDIT_TYPE == 3).astype(int)\n",
    "test['has_potreb'] = (test.CREDIT_TYPE == 3).astype(int)\n",
    "\n",
    "df_train['num_micro_credit'] = df_train.ID.map(train.groupby('ID')['has_micro_credit'].sum())\n",
    "df_test['num_micro_credit'] = df_test.ID.map(test.groupby('ID')['has_micro_credit'].sum())\n",
    "df_train['num_ipoteka'] = df_train.ID.map(train.groupby('ID')['has_ipoteka'].sum())\n",
    "df_test['num_ipoteka'] = df_test.ID.map(test.groupby('ID')['has_ipoteka'].sum())\n",
    "df_train['num_potreb'] = df_train.ID.map(train.groupby('ID')['has_potreb'].sum())\n",
    "df_test['num_potreb'] = df_test.ID.map(test.groupby('ID')['has_potreb'].sum())\n",
    "\n",
    "df_train['ratio_micro_credit'] = df_train.ID.map(train.groupby('ID')['has_micro_credit'].mean())\n",
    "df_test['ratio_micro_credit'] = df_test.ID.map(test.groupby('ID')['has_micro_credit'].mean())\n",
    "df_train['ratio_ipoteka'] = df_train.ID.map(train.groupby('ID')['has_ipoteka'].mean())\n",
    "df_test['ratio_ipoteka'] = df_test.ID.map(test.groupby('ID')['has_ipoteka'].mean())\n",
    "df_train['ratio_potreb'] = df_train.ID.map(train.groupby('ID')['has_potreb'].mean())\n",
    "df_test['ratio_potreb'] = df_test.ID.map(test.groupby('ID')['has_potreb'].mean())\n",
    "\n",
    "for col in ['has_micro_credit', 'has_ipoteka', 'has_potreb']:\n",
    "    new_col = col + '_weighted'\n",
    "    train['tmp'] = train[col] * train['weight']\n",
    "    test['tmp'] = test[col] * test['weight']\n",
    "    df_train[new_col] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test[new_col] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in fstats:\n",
    "    df_train['amt_annuity_{}'.format(func.__name__)] = df_train.ID.map(train.groupby('ID')['AMT_ANNUITY'].agg(func))\n",
    "    df_test['amt_annuity_{}'.format(func.__name__)] = df_test.ID.map(test.groupby('ID')['AMT_ANNUITY'].agg(func))\n",
    "\n",
    "    df_train['amt_annuity_nonzero_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.loc[train.AMT_ANNUITY > 0].groupby('ID')['AMT_ANNUITY'].agg(func))\n",
    "    df_test['amt_annuity_nonzero_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.loc[test.AMT_ANNUITY > 0].groupby('ID')['AMT_ANNUITY'].agg(func))\n",
    "\n",
    "col = 'AMT_ANNUITY'\n",
    "new_col = 'amt_annuity_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in fstats:\n",
    "    df_train['amt_annuity_current_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.loc[train.DTIME_CREDIT_ENDDATE > train.SK_DATE_DECISION].groupby('ID')['AMT_ANNUITY'].agg(func))\n",
    "    df_test['amt_annuity_current_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.loc[test.DTIME_CREDIT_ENDDATE > test.SK_DATE_DECISION].groupby('ID')['AMT_ANNUITY'].agg(func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['AMT_REQ'] = train[[x for x in train.columns if x.startswith('AMT_REQ_SOURCE')]].sum(axis=1)\n",
    "test['AMT_REQ'] = test[[x for x in train.columns if x.startswith('AMT_REQ_SOURCE')]].sum(axis=1)\n",
    "\n",
    "for func in fstats:\n",
    "    df_train['num_requests_{}'.format(func.__name__)] = df_train.ID.map(train.groupby('ID')['AMT_REQ'].agg(func))\n",
    "    df_test['num_requests_{}'.format(func.__name__)] = df_test.ID.map(test.groupby('ID')['AMT_REQ'].agg(func))\n",
    "    \n",
    "col = 'AMT_REQ'\n",
    "new_col = 'num_requests_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in fstats:\n",
    "    df_train['num_prolong_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.groupby('ID')['CNT_CREDIT_PROLONG'].agg(func)\n",
    "    )\n",
    "    df_test['num_prolong_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.groupby('ID')['CNT_CREDIT_PROLONG'].agg(func)\n",
    "    )\n",
    "    \n",
    "col = 'CNT_CREDIT_PROLONG'\n",
    "new_col = 'num_prolong_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "    \n",
    "for func in fstats:\n",
    "    df_train['amt_max_overdue_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.groupby('ID')['AMT_CREDIT_MAX_OVERDUE'].agg(func)\n",
    "    )\n",
    "    df_test['amt_max_overdue_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.groupby('ID')['AMT_CREDIT_MAX_OVERDUE'].agg(func)\n",
    "    )\n",
    "    \n",
    "col = 'AMT_CREDIT_MAX_OVERDUE'\n",
    "new_col = 'amt_max_overdue_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "    \n",
    "for func in fstats:\n",
    "    df_train['amt_sum_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.groupby('ID')['AMT_CREDIT_SUM'].agg(func)\n",
    "    )\n",
    "    df_test['amt_sum_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.groupby('ID')['AMT_CREDIT_SUM'].agg(func)\n",
    "    )    \n",
    "\n",
    "col = 'AMT_CREDIT_SUM'\n",
    "new_col = 'amt_sum_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "    \n",
    "for func in fstats:\n",
    "    df_train['amt_sum_debt_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.groupby('ID')['AMT_CREDIT_SUM_DEBT'].agg(func)\n",
    "    )\n",
    "    df_test['amt_sum_debt_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.groupby('ID')['AMT_CREDIT_SUM_DEBT'].agg(func)\n",
    "    )\n",
    "    \n",
    "col = 'AMT_CREDIT_SUM_DEBT'\n",
    "new_col = 'amt_sum_debt_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "    \n",
    "for func in fstats:\n",
    "    df_train['amt_sum_limit_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.groupby('ID')['AMT_CREDIT_SUM_LIMIT'].agg(func)\n",
    "    )\n",
    "    df_test['amt_sum_limit_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.groupby('ID')['AMT_CREDIT_SUM_LIMIT'].agg(func)\n",
    "    )\n",
    "    \n",
    "col = 'AMT_CREDIT_SUM_LIMIT'\n",
    "new_col = 'amt_sum_limit_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "    \n",
    "for func in fstats:\n",
    "    df_train['amt_sum_overdue_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.groupby('ID')['AMT_CREDIT_SUM_OVERDUE'].agg(func)\n",
    "    )\n",
    "    df_test['amt_sum_overdue_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.groupby('ID')['AMT_CREDIT_SUM_OVERDUE'].agg(func)\n",
    "    )\n",
    "    \n",
    "col = 'AMT_CREDIT_SUM_OVERDUE'\n",
    "new_col = 'amt_sum_overdue_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in fstats:\n",
    "    df_train['sum_type_{}'.format(func.__name__)] = df_train.ID.map(\n",
    "        train.groupby('ID')['CREDIT_SUM_TYPE'].agg(func)\n",
    "    )\n",
    "    df_test['sum_type_{}'.format(func.__name__)] = df_test.ID.map(\n",
    "        test.groupby('ID')['CREDIT_SUM_TYPE'].agg(func)\n",
    "    )\n",
    "    \n",
    "col = 'CREDIT_SUM_TYPE'\n",
    "new_col = 'sum_type_weighted'\n",
    "train['tmp'] = train[col] * train['weight']\n",
    "test['tmp'] = test[col] * test['weight']\n",
    "df_train[new_col] = df_train.ID.map(\n",
    "    train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    ")\n",
    "df_test[new_col] = df_test.ID.map(\n",
    "    test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CREDIT_DELAY_ALL'] = train[[x for x in train.columns if x.startswith('CREDIT_DELAY')]].sum(axis=1)\n",
    "test['CREDIT_DELAY_ALL'] = test[[x for x in test.columns if x.startswith('CREDIT_DELAY')]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [x for x in train.columns if x.startswith('CREDIT_DELAY')]:\n",
    "    for func in fstats:\n",
    "        df_train['{}_{}'.format(col.lower(), func.__name__)] = df_train.ID.map(\n",
    "            train.groupby('ID')[col].agg(func)\n",
    "        )\n",
    "        df_test['{}_{}'.format(col.lower(), func.__name__)] = df_test.ID.map(\n",
    "            test.groupby('ID')[col].agg(func)\n",
    "        )\n",
    "        \n",
    "    new_col = col.lower() + '_weighted'\n",
    "    train['tmp'] = train[col] * train['weight']\n",
    "    test['tmp'] = test[col] * test['weight']\n",
    "    df_train[new_col] = df_train.ID.map(\n",
    "        train.groupby('ID')['tmp'].mean() / train.groupby('ID')['sum_weight'].first()\n",
    "    )\n",
    "    df_test[new_col] = df_test.ID.map(\n",
    "        test.groupby('ID')['tmp'].mean() / test.groupby('ID')['sum_weight'].first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь сохраним данные от первого прохода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_active = df_train.copy()\n",
    "df_test_active = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Внимание__. Теперь необходимо вернуться назад и заново загрузить датасет, сделать второй проход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединение признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.set_index('ID', inplace=True)\n",
    "df_test.set_index('ID', inplace=True)\n",
    "df_train_active.set_index('ID', inplace=True)\n",
    "df_test_active.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat((df_train, df_train_active.rename(columns={x:x+'_active' for x in df_train_active.columns})), axis=1)\n",
    "df_test = pd.concat((df_test, df_test_active.rename(columns={x:x+'_active' for x in df_test_active.columns})), axis=1)\n",
    "\n",
    "df_train.drop('DEF_active', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим несколько моделей с различными отложенными выборками и параметрами, усредним:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params = [\n",
    "    {'lambda_l2': 1.1391988554694428e-40, 'num_leaves': 15},\n",
    "    {'lambda_l2': 1.0, 'num_leaves': 26},\n",
    "]\n",
    "\n",
    "ltrain = lgb.Dataset(df, target)\n",
    "\n",
    "for i, params in enumerate(list_params):\n",
    "    params['objective'] = 'binary'\n",
    "    params['metric'] = 'auc'\n",
    "    params['learning_rate'] = 0.02\n",
    "    \n",
    "    df_test['prediction_{}'.format(str(i))] = 0\n",
    "    \n",
    "    scores = []\n",
    "    for i_tr, i_ts in cv.split(df):\n",
    "        X_tr = df.iloc[i_tr]\n",
    "        X_ts = df.iloc[i_ts]\n",
    "        y_tr = target[i_tr]\n",
    "        y_ts = target[i_ts]\n",
    "        ltrain = lgb.Dataset(X_tr, y_tr)\n",
    "        lvalid = ltrain.create_valid(X_ts, y_ts)\n",
    "        bst = lgb.train(params, train_set=ltrain, valid_sets=lvalid,\n",
    "                        num_boost_round=2000, early_stopping_rounds=50,\n",
    "                        verbose_eval=False)\n",
    "        y_pred = bst.predict(X_ts)\n",
    "        scores.append(roc_auc_score(y_ts, y_pred))    \n",
    "        y_pr = bst.predict(df_test)\n",
    "        df_test['prediction_{}'.format(str(i))] += y_pr / 5.0\n",
    "        \n",
    "    print np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Сохраним предсказания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[[x for x in df_test.columns if x.startswith('prediction_')]].corr()\n",
    "df_test['Score'] = df_test[[x for x in df_test.columns if x.startswith('prediction_')]].mean(axis=1)\n",
    "df_test[['ID', 'Score']].to_csv('../output/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
